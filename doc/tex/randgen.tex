\chapter{Random number generation}
\label{chap:randgen}

The ability to generate pseudo-random numbers (random number
generation, or RNG) is indispensable for modern statistical
methods. In this chapter we provide some details on the functions
gretl offers in this area, with a brief description of the most
important algorithms it uses.

\section{Uniform and normal pseudo-random variates}
\label{sec:randgen-uni-norm}

The cornerstone of pseudo-random number generation is the ability to
generate high-quality uniform variates, that is, random numbers $U$ in
the unit interval such that $P(a \le U < b) = b - a$ for
$0 <a < b < 1$. As is well known, pseudo-random numbers generated by
software such as gretl are not truly random. They are practically
indistinguishable from genuine random numbers, but are in fact
generated sequentially via a deterministic algorithm.

This means that one can set the starting point of the sequence and
obtain a repeatable sequence of random numbers. Each algorithm has a
\emph{cycle length}, or \emph{period}, the number of draws after which
the sequence starts repeating itself. Clearly, the longer the cycle
length, the better, \emph{ceteris paribus}, is the algorithm.  The
issue was analyzed in great detail in \cite{SvenTalha2012}, in which
the quality of the generated pseudo-random numbers, in term of
simulating genuine random draws, was found to be very good. The RNG
that gretl now uses for generating uniforms is the SIMD-oriented Fast
Mersenne Twister (\textsf{SFMT}) by \cite{saito_matsumoto08}; its
period is a nonzero multiple of $2^{19937} - 1$.

The starting point of the sequence is also called the \emph{seed} of
the generator, and it can be set by using the \cmd{set seed} command.
The seed must be expressible as a 32-bit unsigned integer, lying
between 0 and $2^{32} - 1 = 4294967295$. If the user does not provide
a seed, gretl automatically sets it to the so-called ``Unix time'',
that is the number of seconds elapsed between 1970-01-01 00:00:00 UTC
and the time at which gretl was started.

The `raw' output of the Mersenne Twister consists of pseudo-random
unsigned integers from the uniform distribution on the interval
$[0, 2^{32}-1]$. These are unlikely to be wanted in econometric work;
since all numeric data in gretl is stored in double precision format,
the Mersenne Twister is used for producing real numbers between 0 and
1 on an equispaced grid of $2^{53}$ values, which roughly corresponds
to 16 decimal digits.

Gretl offers two functions for producing uniform variates on
$[0,1]$: \cmd{uniform}, which produces a series, and \cmd{muniform},
which produces a matrix, (see the \GCR\ for the syntax details).  Of
course, the former function can be used only if a dataset is in place.

For example, the code
\begin{code}
  set seed 1234567890
  eval muniform(3,2)
\end{code}
in which the seed is set by the user, should invariably produce the
matrix
\begin{code}
     0.43600      0.90402
     0.46097      0.49748
     0.26473      0.63962
\end{code}
If you wish to reset the seed to something over which you have no
control, you can use the command \cmd{set seed auto}.

The second most important distribution in econometric computation is
the normal distribution: gretl offers the \cmd{normal} function for
series and the \cmd{mnormal} function for matrices, which produce
standard Gaussian variates. The algorithm we use is the ``Ziggurat''
method originally devised by \cite{marsaglia00}; gretl has adopted the
refinements introduced by David Bateman in the \textsf{Octave}
implementation. The normal equivalent to the code above is
\begin{code}
  set seed 1234567890
  eval mnormal(3,2)
\end{code}
whose output is
\begin{code}
   -0.019792       1.6853
     -1.5568      0.85319
      1.2661     -0.90943
\end{code}

By the linearity property of the normal distribution, it is easy to
draw from a generic normal distribution with arbitrary mean and
covariance matrix. If you need pseudo-random numbers drawn from a
normal density with mean $\mu$ and covariance matrix $\Sigma$, you can
perform an affine transformation, as in
\[
  X \sim N(0, I) \Longrightarrow AX + b  \sim N(b, AA')
\]
Therefore to draw from a $N(\mu, \Sigma)$ distribution you need a
matrix whose product by its own transpose equals $\Sigma$, the
Cholesky decomposition being a popular choice. For example,
\begin{code}
  set seed 1234567890
  scalar N = 8192
  matrix X = mnormal(N,2)
  matrix m = {3,4}
  matrix S = {4,2;2,2}
  matrix K = cholesky(S)
  matrix Z = X * K' .+ m
  eval meanc(Z)
  eval mcov(Z)
\end{code}
will produce
\begin{code}
  2.9727       3.9924

  4.0690       2.0670
  2.0670       2.0406
\end{code}
Note that in this example the individual draws are the \emph{rows} of the
matrices $X$ and $Z$, so we are using a row vector $m$ and the
transpose of the Cholesky factor $K$. For more details on matrix
operations, see Chapter \ref{chap:matrices} in the \GUG.

When you use the \cmd{normal} function for producing a series, the
above transformation can be performed via two optional arguments,
denoting mean and standard deviation, respectively. The following code
demonstrates the equivalence of the two methods:
\begin{code}
set seed 1234567890
nulldata 8192
scalar m = 3
scalar s = 2
series x1 = normal() * s + m
series x2 = normal(m, s)
summary x1 x2 --simple
\end{code}
The output of the above is
\begin{code}
          Mean      Median        S.D.         Min         Max
x1      2.9727      2.9781      2.0172     -4.2671      11.460
x2      3.0121      3.0019      1.9909     -5.4445      10.923
\end{code}

\section{Other distributions via native functions}
\label{sec:randgen-other-native}

Gretl offers several built-in algorithms for generating pseudo-rvs
efficiently from a reasonably large array of distributions. These
algorithms are accessed via three different functions, which share a
similar syntax, that is
\begin{flushleft}
  \emph{\texttt{function(distributional arguments[, dimensional arguments])}}
\end{flushleft}
where \emph{\texttt{function}} can be
\begin{itemize}
\item \cmd{randgen1} for generating scalars,
\item \cmd{randgen} for generating series and
\item \cmd{mrandgen} for generating matrices.
\end{itemize}

The distributional arguments have to be passed first, to identify the
distribution from which you want to draw numbers. The first one must
be a string identifying the distribution; extra numerical parameters
may be needed to pin down its exact shape (e.g.\ one
degrees-of-freedom term for the $\chi^2$ density and two for the $F$
density). For example, suppose you want to generate a single draw from
a $t$ distribution with 12 degrees of freedom; the command would be
something like
\begin{code}
  scalar foo = randgen1(t, 12)
\end{code}
where \texttt{t} identifies the Student's $t$ density. The
\cmd{mrandgen} function needs two additional parameters (the
\emph{\texttt{dimensional arguments}} noted above) for the size of the
returned matrix. The following example generates a $4 \times 3$ matrix
of pseudo-random numbers from a Poisson distribution with parameter 2:
\begin{code}
  matrix bar = mrandgen(P, 2, 4, 3)
\end{code}

Tables \ref{tab:discrete} and \ref{tab:continuous} contain a summary
of the supported distributions and their arguments.\footnote{In both
  tables, $B(a,b) = \frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)}$ is 
    Euler's Beta function.}

\begin{table}[htbp]
\centering
\begin{tabular}{rccl}
  \hline
  distribution	& code	& $P(x) = k$	& extra parameters \\
  \hline
Discrete uniform	& \texttt{i}	& $\frac{1}{M - m +1}$	& $m, M$ \\[6pt]
Binomial		& \texttt{b}	& ${n \choose k} p^k (1-p)^{(n-k)}$	& $p, n$ \\[6pt]
Poisson			& \texttt{p}	& $e^{-\mu} \frac{\mu^k}{k!}$	& $\mu$ \\[6pt]
Beta-Binomial		& \texttt{bb}	& ${n \choose k} \frac{B(k+\alpha,n-k+\beta)}
                          {B(\alpha,\beta)}$ & $n, \alpha$, $\beta$ \\[6pt]
Generic discrete	& \texttt{disc}	& $p_k$	& $\mathbf{p}$ \\[6pt]
  \hline
  \end{tabular}
\caption{Discrete distributions}
\label{tab:discrete}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{rccl}
  \hline
  distribution	& code	& $f(x)$	& extra parameters \\
  \hline
  Uniform	& \texttt{u}
      & $\frac{1}{b-a}$	&  $a,b$\\[9pt]
  Normal	& \texttt{z}
      & $\frac{1}{\sigma \sqrt{2\pi}}
      \exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right]$

      & $\mu, \sigma$ \\[9pt]
  Student's $t$	& \texttt{t}
      & $\frac{\Gamma{\left(\frac{\nu + 1}{2}\right)}}{\sqrt{\pi\nu}\,
        \Gamma{\left(\frac{\nu}{2}\right)}}
        \left(1 + \frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}$
      & $\nu$ \\[9pt]
  $\chi^2$	& \texttt{x}
      & $\frac{1}{2^{p/2}\Gamma(p/2)}\; x^{(p/2)-1} e^{-x/2}$
      &  $p$ \\[9pt]
  Snedecor's $F$	& \texttt{f}
      & $\dfrac{(d_1/d_2)^{d_1/2} x^{(d_1/2)-1}}
       {B(d_1/2, d_2/2)[1+(d_1x/d_2)]^{(d_1+d_2)/2}}$
      & $d_1, d_2$ \\[9pt]
  Exponential	& \texttt{exp}
      & $\frac{e^{-x/\mu}}{\mu}$	& $\mu$ \\[9pt]
  Weibull	& \texttt{w}
      & $\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}
        e^{-(x/\lambda)^{k}}$
      & $k, \lambda$ \\[9pt]
  Gamma	& \texttt{g}
      & $\frac{ \alpha^{-p}}{\Gamma(p)} x^{p - 1} e^{-x/\alpha}$

      & $p, \alpha$ \\[9pt]
  Generalized Error	& \texttt{e}
      & $\frac{\nu}{2^{1+1/\nu} \Gamma(1/\nu)} \exp[{-\frac{1}{2}|x|^\nu}]$
      & $\nu$  \\[9pt]
  Laplace	& \texttt{l}
     & $\frac{1}{2 h} \exp[{-\frac{|x - \mu|}{h}}]$
     & $\mu, h$ \\[9pt]
  Beta	& \texttt{beta}
     & $\frac{1}{B(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}$
     & $\alpha, \beta$	\\[9pt]
  Logistic	& \texttt{s}
     & $\frac{e^{-(x-\mu)/s}} {s\left(1+e^{-(x-\mu)/s}\right)^2}$

     & $\mu, s$ \\
  \hline
  \end{tabular}
\caption{Continuous distributions}
\label{tab:continuous}
\end{table}

The apparatus above does not cover two special distributions that are
also available in \textsf{gretl}, the Dirichlet and Wishart
distributions.

Since the Dirichlet distribution is multivariate, draws from the
Dirichlet distributions can be obtained using the \cmd{mrandgen}
function with \texttt{dir} as its first argument. The second argument
must be a vector of $p$ distributional parameters and the third one
specifies how many draws the user wants. It returns a matrix with $p$
columns, in which each row is an independent draw.

For the Wishart distribution we have the dedicated function
\cmd{iwishart}, which returns draws from the \emph{inverse} of a
Wishart-distributed random matrix using the method of
\cite{odell-feiveson66}.


\section{Other distributions via scripting}
\label{sec:randgen-other-hansl}

Other distributions can often be handled by methods that are fairly
straightforward to code in hansl: a simple and general one is the
``inversion method'': if $X$ is a random variable with distribution
function $F$, then $U = F(X)$ is a uniform random variable. Therefore,
one method for generating variates from a distribution with function
$F$---assuming that the inverse function $F^{-1}$ is readily
computable---is to generate uniform variates and apply the inverse
transformation:
\[
  X = F^{-1}(U) .
\]
For example, take a Beta distribution with parameter $\alpha = 2$ and
$\beta=1$, that is
\[
  f(x) = 2 x
\]
Its distribution function can be written as
\[
  F(z) = P(x \le z) = \int_{0}^z 2 x\ \mathrm{d}x = \left[ x^2\right]_0^z = z^2;
\]
therefore, by inverting this function, it follows that $X$ must be
such that
\[
  U = X^2,
\]
where $U$ is a uniform random variable, so
\[
  X = \sqrt{U}
\]
will follow the desired distribution, and the hansl command to
generate $X$ is simply
\begin{code}
  series x = sqrt(uniform())
\end{code}

% For example, take the geometric distribution with parameter $\alpha$,
% that is
% \[
%   p(x) = (1-\alpha) \cdot \alpha^x
% \]
% Its distribution function can be written as
% \[
%   F(z) = P(x \le z) = (1-\alpha) \sum_{x=0}^z \alpha^x = (1-\alpha)
%   \frac{1-\alpha^{z+1}}{1-\alpha} = 1-\alpha^{z+1} ;
% \]
% therefore, by inverting this function, it follows that $X$ must be
% such that
% \[
%   1 - \alpha^{X+1} = U^*,
% \]
% where $U^*$ is a uniform random variable; but then, if $U^* \sim
% U(0,1)$, the same goes for $U = 1 - U^*$. Therefore,
% \[
%   \alpha^{X+1} = U \Longrightarrow (X+1) \log \alpha = \log U
% \]
% \[
%   X = \left\lfloor \frac{\log{U}}{\log{\alpha}} \right\rfloor
% \]
% will follow a geometric distribution. A corresponding script is
% \begin{code}
%   nulldata 1024
%   alpha = 0.75
%   series x = floor(log(uniform())/log(alpha))
% \end{code}

More complex methods, such as acceptance--rejection or
ratio-of-uniforms, can also be easily implemented as hansl
functions. For further details, see \cite{Gentle2004},
\cite{Monahan2001}, chapter 11 or \cite{Tanizaki2004}, chapters 2--3.

\section{Usage with MPI}
\label{sec:mpi}

Some special issues arise when a task that involves generating random
numbers is parallelized across several processes, as in \textsf{MPI}
(Message Passing Interface). Details on gretl's support for
\textsf{MPI}, and how to use it, can be found in \cite{cottrell-mpi};
here we'll just say a little about the RNG aspect.

If the design of the task is such that one process generates all the
random variates, which are then ``broadcast'' or ``scattered'' to the
other processes then it's OK to use the \textsf{SMFT} as
usual. However, if each process is supposed to generate its own random
variates it's not desirable for them all to use the \textsf{SFMT}, with
different seeds: this can end up producing sequences with arbitrary
dependency. To get around that problem gretl offers an alternative,
namely the use of the \textsf{DCMT} mechanism (Dynamic Creation of
Mersenne Twisters) so that each \textsf{MPI} process gets its own,
independent PRNG.\footnote{See
  \url{https://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/DC/dc.html}.}
Use of \textsf{DCMT} is in fact the default when gretl is working in
\textsf{MPI} mode, but use of a single RNG can be forced by use of
the \verb|--single-rng| option, or by manipulating the \dtk{use_dcmt}
state variable via the \texttt{set} command.

In fact this is not very complicated from the user's point of view,
since use of the various functions discussed above proceeds in
exactly the same way, regardless of whether the underlying engine for
generating pseudo-random unsigned integers is \textsf{SFMT} or
\textsf{DCMT}.

\section{The Gibbs sampler}
\label{sec:gibbs}

It's probably no exaggeration to say that the surge in popularity
Bayesian methods have had from the 1990s onwards in largely due to the
possibility of generating pseudo random numbers from a very wide range
of (possibly very intricate) distributions. This was made possible by
the availability of cheap, fast hardware but, most importantly, of
efficient sampling algorithms. One of the most celebrated ones is the
\emph{Gibbs sampler} (insert references --- surely \cite{casella92},
but possibly others).

Roughly speaking, the Gibbs sampler is used for sampling in a rather
common situation, where you have a multivariate distribution in which
the marginal distributions may be complicated to write, but the
\emph{conditional} distribution of each of its elements is a simple
one, \emph{conditional on the other ones}.

The algorithm is iterative, making it a special case of MCMC. For
example, suppose we want to generate values for three rv $(X, Y,
Z)$. The algorithm proceeds as follows:
\begin{enumerate}
\item set $X_0, Y_0$ and $Z_0$ to something in their respective support.
\item Draw $X_i$ from $f(X|Y_{i-1}, Z_{i-1})$
\item Draw $Y_i$ from $f(Y|X_i, Z_{i-1})$
\item Draw $Z_i$ from $f(Z|X_i, Y_i)$
\item repeat from 2 many times
\item discard the first draws (the so-called \emph{burn-in} stage)
\end{enumerate}

What you will get in the end is draws from the unconditional densities
of $X$, $Y$ and $Z$. For example, suppose one wants to draw from a
bivariate normal distibution with the following parameters:
\[
  \left( \begin{array}{c} Y \\ X \end{array} \right) \sim
  N \left[
  \left( \begin{array}{c} 3 \\ 1 \end{array} \right), 
  \left( \begin{array}{cc} 2 & 1 \\ 1 & 4 \end{array} \right)
  \right]
\]
Of course, it is perfectly possible (and numerically preferable) to
use the direct method described in Section \ref{sec:randgen-uni-norm},
but if one wanted to use the Gibbs sampler, the starting point would
be writing down the conditional distributions:
\begin{eqnarray*}
  Y|X & \sim & N\left( 3 + \frac{X-1}{4},\  2 - \frac{1}{4} \right) \\
  X|Y & \sim & N\left( 1 + \frac{Y-3}{2},\  4 - \frac{1}{2} \right)
\end{eqnarray*}

The tool \app{gretl} offers is the ``gibbs block'', that is a block of
statements which begins with \cmd{gibbs} and ends with \cmd{end
  gibbs}. 

On the first line, the number of draws and the burn-in period must be
indicated via the \cmd{N} and \cmd{burnin} keywords, either as
numerical values or via the names of pre-existing scalar variables). The 
the \cmd{output} argument indicates the name of the bundle that will
eventually contain the results.

The interior lines of the block can have a keyword prepended, either
init or record, with the following meanings.
\begin{itemize}
\item \cmd{init} identifies a statement as an initializer, to be
  executed just once prior to the Gibbs iteration.
\item \cmd{record} tags a statement as one whose result should be
  recorded at each iteration.
\end{itemize}

Statements that feature neither keyword are taken to be part of the
iteration but their results are not recorded in the output
bundle. Statements tagged with init should be given first.

The primary content of the output bundle is a matrix named \texttt{H}
which by default has $N$ rows and a column dimension that depends on
the number and type of the marginals to be recorded; matrix results
are recorded in transposed vec form. Besides this matrix the bundle
contains several items of metadata which make it a self-contained
record of the sampler. If the bundle named in the output argument
already exists it is overwritten, otherwise a new bundle is created.

The code for generating a double normal follows:

\begin{code}
nulldata 10000
set seed 1234567890

ndraws = $nobs
gibbs burnin=100 N=ndraws output=b
    init x = 0
    m = (x-1)/4 + 3
    s = sqrt(2 - 1/4)
    record y = randgen1(z, m, s) 
    m = (y-3)/2 + 1
    s = sqrt(4 - 1/2)
    record x = randgen1(z, m, s)
end gibbs

series Y = b.H[,1]
series X = b.H[,2]
\end{code}
%$
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{figures/dnormal-density.png}
  \caption{Estimated density of the generated data}
  \label{fig:dnormal-density}
\end{figure}

Here's the output (Figure \ref{fig:dnormal-density} displays a kernel
estimate of the joint density of the generated data).

\begin{code}

scalar y
             Mean      Median        S.D.         Min         Max
col1       3.0017      2.9967      1.4172     -2.2430      8.6539

scalar x
             Mean      Median        S.D.         Min         Max
col2      0.99380     0.99801      1.9994     -5.8432      9.1098
  
\end{code}

The sequential nature of the algorithm does not guarantee that the
draws will be serially independent: in the example above, $Y_i$ and
$Y_{i+1}$ are drawn from the same marginal distribution, but they need
not necessarily be independent, although the Markov nature of the
Gibbs sampler guarantees that the degree of mutual dependence between
$Y_i$ and $Y_{i+k}$ should decrease with $k$. For example, Figure
\ref{fig:Y-ACF} displays the autocorrelation function of the values
for $Y$ generated by the previous example.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.75]{figures/Y-ACF}
  \caption{Autocorrelation function of the generated $Y$}
  \label{fig:Y-ACF}
\end{figure}

In order to overcome this problem, in some cases the algorithm works
by retaining only a fraction of the draws obtained via the algorithm
above and skipping the ones in between. In practice, (\ldots
illustrate \option{thinning} \ldots)

\ldots possibly, more complex examples \ldots

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "gretl-guide"
%%% End:
